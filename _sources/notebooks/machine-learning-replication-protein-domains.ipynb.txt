{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(machine-learning-replication-protein-domains)=\n",
    "\n",
    "# Machine learning algorithm based on protein domains\n",
    "\n",
    "## Importing libraries and datasets\n",
    "\n",
    "- Replication of results from paper: \"Predicting yeast synthetic lethal genetic interactions using protein domains\"\n",
    "    - Authors: Bo Li, Feng Luo,School of Computing,Clemson University,Clemson, SC, USA\n",
    "    - e-mail: bol, luofeng@clemson.edu\n",
    "    - year:2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict \n",
    "import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "import scipy as scipy\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download datasets from this github repo ðŸ‘‡\n",
    "\n",
    "The link: https://github.com/leilaicruz/machine-learning-for-yeast/tree/dev_Leila/datasets-for-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_dir = os.path.dirname('__file__') #<-- absolute dir the script is in\n",
    "rel_path_SL = \"datasets/data-synthetic-lethals.xlsx\"\n",
    "rel_path_nSL=\"datasets/data-positive-genetic.xlsx\"\n",
    "rel_path_domains=\"datasets/proteins-domains-from-Pfam.xlsx\"\n",
    "\n",
    "abs_file_path_SL = os.path.join(script_dir, rel_path_SL)\n",
    "abs_file_path_nSL = os.path.join(script_dir, rel_path_nSL)\n",
    "abs_file_path_domains = os.path.join(script_dir, rel_path_domains)\n",
    "\n",
    "\n",
    "os.chdir('mini_book/docs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Datasets \n",
    "\n",
    "my_path_sl= abs_file_path_SL\n",
    "my_path_non_sl=abs_file_path_nSL\n",
    "my_path_domains=abs_file_path_domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sl=pd.read_excel(my_path_sl,header=0)\n",
    "data_domains=pd.read_excel(my_path_domains,header=0,index_col='Unnamed: 0')\n",
    "data_domains=data_domains.dropna()\n",
    "data_nonsl=pd.read_excel(my_path_non_sl,header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the feature matrix\n",
    "One matrix for true SL where each row is one pair of SL. Every raw will be a vector of 0,1 or 2 depending on the comparison with the domain list. For row i the jth element = 0 if the jth element of the domain list is not in neither protein A and B, 1, if it is in one of them and 2 if it is in both of them .\n",
    "\n",
    "### Building the list of proteins domains id per protein pair separately :\n",
    "List of protein A: Search for the Sl/nSL database the query gene name and look in the protein domain database which protein domains id has each of those queries.\n",
    "List of protein B: Search for the Sl/nSL database the target gene name of the previous query and look in the protein domain database which protein domains id has each of those target genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the meaningful columns in the respective dataset\n",
    "domain_id_list=data_domains['domain-name']\n",
    "query_gene=data_sl['gene-query-name']\n",
    "target_gene=data_sl['gene-target-name']\n",
    "query_gene_nonlethal=data_nonsl['gene-query-name']\n",
    "target_gene_nonlethal=data_nonsl['gene-target-name']\n",
    "\n",
    "\n",
    "\n",
    "# Initialising the arrays\n",
    "protein_a_list=[]\n",
    "protein_b_list=[]\n",
    "protein_a_list_non=[]\n",
    "protein_b_list_non=[]\n",
    "\n",
    "population = np.arange(0,len(data_sl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now , a for loop for 4000 pairs (minimum for an proper fit, due to that we have 3026 features) sampled randomly from the SL/nSl pair list and creating a big array of protein domains id per protein pair.\n",
    "**THIS IS JUST AN EXAMPLE  WITH A SMALL SAMPLE FOR SHOWING PURPOSES** I usually use 10000 protein pairs, but is highly timing consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in random.sample(list(population), 4000):\n",
    "    protein_a=data_domains[data_domains['name']==query_gene[m]]\n",
    "    protein_b=data_domains[data_domains['name']==target_gene[m]]\n",
    "    protein_a_list.append(protein_a['domain-name'].tolist())\n",
    "    protein_b_list.append(protein_b['domain-name'].tolist())\n",
    "\n",
    "    protein_a_non=data_domains[data_domains['name']==query_gene_nonlethal[m]]\n",
    "    protein_b_non=data_domains[data_domains['name']==target_gene_nonlethal[m]]\n",
    "    protein_a_list_non.append(protein_a_non['domain-name'].tolist())\n",
    "    protein_b_list_non.append(protein_b_non['domain-name'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We are going to analyze',len((protein_a_list)) ,'protein pairs, out of',len(data_sl),'SL protein pairs')\n",
    "\n",
    "print('We are going to analyze',len((protein_a_list_non)) ,'protein pairs, out of',len(data_nonsl),'positive protein pairs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove protein pairs from study if either protein in the pair does not contain any domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_domains(protein_list_search,protein_list_pair):\n",
    "    index=[]\n",
    "    for i in np.arange(0,len(protein_list_search)):\n",
    "        if protein_list_search[i]==[]:\n",
    "            index.append(i) ## index of empty values for the protein_a_list meaning they dont have any annotated domain\n",
    "\n",
    "    y=[x for x in np.arange(0,len(protein_list_search)) if x not in index] # a list with non empty values from protein_a list\n",
    "\n",
    "    protein_list_search_new=[]\n",
    "    protein_list_pair_new=[]\n",
    "    for i in y:\n",
    "        protein_list_search_new.append(protein_list_search[i])\n",
    "        protein_list_pair_new.append(protein_list_pair[i])\n",
    "    return protein_list_search_new,protein_list_pair_new\n",
    "\n",
    "## evaluating the function\n",
    "\n",
    "protein_a_list_new,protein_b_list_new=remove_empty_domains(protein_a_list,protein_b_list)\n",
    "protein_a_list_non_new,protein_b_list_non_new=remove_empty_domains(protein_a_list_non,protein_b_list_non)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The empty domain in the SL were:', len(protein_a_list)-len(protein_a_list_new), 'out of', len(protein_a_list),'domains')\n",
    "\n",
    "print('The empty domain in the nSL were:', len(protein_a_list_non)-len(protein_a_list_non_new), 'out of', len(protein_a_list_non),'domains')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select from each ordered indexes of domain id list which of them appear once, in both or in any of the domains of each protein pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_indexes = lambda x, xs: [i for (y, i) in zip(xs, range(len(xs))) if x == y] # a function that give the index of whether a value appear in array or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_building(protein_a_list_new,protein_b_list_new):\n",
    "    x = np.unique(domain_id_list)\n",
    "    protein_feat_true=np.zeros(shape=(len(x),len(protein_a_list_new)))\n",
    "    pair_a_b_array=[]\n",
    "    for i in np.arange(0,len(protein_a_list_new)):\n",
    "        index_a=[]\n",
    "        pair=[protein_a_list_new[i],protein_b_list_new[i]]\n",
    "        pair_a_b=np.concatenate(pair).ravel()\n",
    "        pair_a_b_array.append(pair_a_b)\n",
    "\n",
    "    for i in np.arange(0,len(pair_a_b_array)):  \n",
    "        array,index,counts=np.unique(pair_a_b_array[i],return_index=True,return_counts=True)\n",
    "        for k,m in zip(counts,array):\n",
    "            if k ==2:\n",
    "                protein_feat_true[get_indexes(m,x),i]=2\n",
    "                \n",
    "            if k==1:\n",
    "                protein_feat_true[get_indexes(m,x),i]=1\n",
    "            # print(index_a[m],i)\n",
    "    return protein_feat_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_feat_true=feature_building(protein_b_list_new=protein_b_list_new,protein_a_list_new=protein_a_list_new)\n",
    "protein_feat_true_pd=pd.DataFrame(protein_feat_true.T)\n",
    "\n",
    "protein_feat_non_true=feature_building(protein_b_list_new=protein_b_list_non_new,protein_a_list_new=protein_a_list_non_new)\n",
    "\n",
    "protein_feat_non_true_pd=pd.DataFrame(protein_feat_non_true.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_2_true=protein_feat_true_pd.where(protein_feat_true_pd==2)\n",
    "index_2_true_count=index_2_true.count(axis=1).sum()\n",
    "\n",
    "index_1_true=protein_feat_true_pd.where(protein_feat_true_pd==1)\n",
    "index_1_true_count=index_1_true.count(axis=1).sum()\n",
    "\n",
    "index_2_nontrue=protein_feat_non_true_pd.where(protein_feat_non_true_pd==2)\n",
    "index_2_nontrue_count=index_2_nontrue.count(axis=1).sum()\n",
    "\n",
    "index_1_nontrue=protein_feat_non_true_pd.where(protein_feat_non_true_pd==1)\n",
    "index_1_nontrue_count=index_1_nontrue.count(axis=1).sum()\n",
    "\n",
    "\n",
    "print('fraction of twos in the SL array is',index_2_true_count/(len(protein_feat_true_pd.index)*len(protein_feat_true_pd.columns)))\n",
    "print('fraction of ones in the SL array is',index_1_true_count/(len(protein_feat_true_pd.index)*len(protein_feat_true_pd.columns)))\n",
    "print('fraction of twos in the PI array is',index_2_nontrue_count/(len(protein_feat_non_true_pd.index)*len(protein_feat_non_true_pd.columns)))\n",
    "print('fraction of ones in the PI array is',index_1_nontrue_count/(len(protein_feat_non_true_pd.index)*len(protein_feat_non_true_pd.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning labels of 0 and 1 if the pair is non lethal or lethal, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_feat_true_pd['lethality']=np.ones(shape=(len(protein_a_list_new)))\n",
    "protein_feat_non_true_pd['lethality']=np.zeros(shape=(len(protein_a_list_non_new)))\n",
    "\n",
    "feature_post=pd.concat([protein_feat_true_pd,protein_feat_non_true_pd],axis=0)\n",
    "feature_post=feature_post.set_index(np.arange(0,len(protein_a_list_new)+len(protein_a_list_non_new)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The number of features are:',feature_post.shape[1])\n",
    "print('The number of samples are:',feature_post.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory visualization of the data\n",
    "\n",
    "### Visualizing how the mean and std depends on whether a pair is SL or nSL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=feature_post.T.describe().loc['mean']\n",
    "std=feature_post.T.describe().loc['std']\n",
    "lethality=feature_post['lethality']\n",
    "\n",
    "corr_keys=pd.concat([mean,std,lethality],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair=sns.pairplot(corr_keys,hue='lethality',diag_kind='kde',kind='reg',palette='colorblind')\n",
    "pair.fig.suptitle('Pairplot to see data dependencies with Lethality',y=1.08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=scipy.stats.pearsonr(corr_keys['mean'],corr_keys['lethality'])\n",
    "p_value_corr=defaultdict(dict)\n",
    "\n",
    "columns=['mean','std']\n",
    "for i in columns:\n",
    "    \n",
    "    tmp=scipy.stats.pearsonr(corr_keys[i],corr_keys['lethality'])\n",
    "    p_value_corr[i]['corr with lethality']=tmp[0]\n",
    "    p_value_corr[i]['p-value']=tmp[1]\n",
    "\n",
    "p_value_corr_pd=pd.DataFrame(p_value_corr)\n",
    "p_value_corr_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = corr_keys.corr()\n",
    "sns.heatmap(corr, vmax=1,vmin=-1 ,square=True,cmap=cm.PRGn,cbar_kws={'label':'Pearson corr'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = feature_post.drop(columns=[\"lethality\"]), feature_post[\"lethality\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X,y,test_size = 0.3, random_state= 0)\n",
    "\n",
    "print ('Train set:', X_train.shape,  y_train.shape)\n",
    "print ('Test set:', X_test.shape,  y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the best SVM model\n",
    "\n",
    "- `from sklearn.model_selection import GridSearchCV`\n",
    "- `from sklearn.svm import SVC`\n",
    "- `parameters = [{'C': [1, 10, 100], 'kernel': ['rbf'], 'gamma': ['auto','scale']}]`\n",
    "- `search = GridSearchCV(SVC(), parameters, n_jobs=-1, verbose=1)`\n",
    "- `search.fit(X_train, y_train)`\n",
    "\n",
    "Then,\n",
    "- `best_parameters = search.best_estimator_`\n",
    "- `print(best_parameters)`\n",
    "\n",
    "I got in this case the following model, to fit the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(C=10, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('The mean squared error is =',metrics.mean_squared_error(y_test,y_pred))\n",
    "print('Test set Accuracy: ', metrics.accuracy_score(y_test, y_pred))\n",
    "print('The Jaccard index is =', jaccard_score(y_test, y_pred))\n",
    "print('The log-loss is =',log_loss(y_test,y_pred))\n",
    "\n",
    "print('The f1-score is =',metrics.f1_score(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Jaccard similarity coefficient, defined as the size of the intersection divided by the size of the union of two label sets. The closer to 1 the better the classifier \n",
    "- Log loss is how far each prediction is from the actual label, it is like a distance measure from the predicted to the actual , the classifier with lower log loss have better accuracy\n",
    "- F1-score:  can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred, target_names=['NonSl','SL']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "scores=clf.decision_function(X_test)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, scores)\n",
    "area=metrics.auc(fpr,tpr)\n",
    "plt.plot(fpr,tpr,color='darkorange',label='SVM model (area = %0.2f)' % area)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--',label='Random prediction')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names=[1,2,3]\n",
    "fig, ax = plt.subplots()\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred,normalize=\"true\")\n",
    "\n",
    "class_names=['SL', 'nSL']\n",
    "\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "sns.heatmap(pd.DataFrame(cm), annot=True, cmap=\"Blues\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = metrics.precision_recall_curve(y_test, scores)\n",
    "average_precision = metrics.average_precision_score(y_test, scores)\n",
    "plt.plot(precision,recall,color='blue',label='SVM-model')\n",
    "\n",
    "plt.plot([0.5, 1], [1, 0], color='navy', lw=2, linestyle='--',label='Random prediction')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('2-class Precision-Recall curve: '\n",
    "                   'AP={0:0.2f}'.format(average_precision))\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
